{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for calculating entropy in bits\n",
    "\n",
    "def entropy(a):\n",
    "    a = np.asarray(a)\n",
    "    a = a/a.sum()\n",
    "    return -np.sum(a*np.log2(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a\n",
    "\n",
    "For a satndard six-side die, there are 6 possible outcome with probability 1/6 for each one,\n",
    "\n",
    "$H(X) = -\\sum_x P(x)log_2P(x) = 6\\times (-\\frac{1}{6}log_2\\frac{1}{6})=2.58 (bits)$\n",
    "\n",
    "# 1b\n",
    "$H(X) = -\\sum_x P(x)log_2P(x) = 2\\times (-0.35\\times log_2 0.35)+2\\times (-0.15\\times log_2 0.15) = 1.88 (bits)$\n",
    "\n",
    "# 1c\n",
    "\n",
    "Since the two dice are independent, there are 6*6=36 outcomes with equal probability 1/36\n",
    "\n",
    "$H(X,Y) = -\\sum_{x,y} P(x,y)log_2 P(x,y) = 36\\times (-\\frac{1}{36}log_2\\frac{1}{36}) = 5.16 (bits) $\n",
    "\n",
    "# 1d\n",
    "By the following properties: 1. $\\sum_{x}P(x) = \\sum_{y}P(y) = 1$ for all random variables. 2. $P(x,y) = P(x)P(y)$ for independent variables X,Y, we can derive $H(X,Y) = H(X) + H(Y)$ as follows:\n",
    "\n",
    "\\begin{align}\n",
    "H(X, Y) & = -\\sum_{x, y} p(x, y)\\log_2(p(x, y)) \\\\ \n",
    "& = -\\sum_{x, y} p(x) p(y)\\log_2(p(x)p(y)) \\\\ \n",
    "& = -\\sum_{x, y} p(x) p(y)\\left[\\log_2(p(x)) + \\log_2(p(y))\\right] \\\\ \n",
    "& = -\\sum_{x, y} p(x) p(y)\\log_2(p(x)) -\\sum_{x, y} p(x) p(y) \\log_2(p(y)) \\\\ \n",
    "& = -\\sum_{x} p(x) \\log_2(p(x)) -\\sum_{y}p(y) \\log_2(p(y))\\\\ \n",
    "& = H(X) + H(Y)\n",
    "\\end{align}\n",
    "\n",
    "As a result, $I(X;Y) = H(X)+H(Y)-H(X,Y) = 0$\n",
    "\n",
    "# 1e\n",
    "Similar as 1d, we can seperate $H(x_1,x_2,...,x_N)$ into $H(x_1) + H(x_2) + ... + H(x_N)$ as\n",
    "\n",
    "\\begin{align}\n",
    "H(x_1,x_2,...,x_N) &= -\\sum_{x_1,x_2,...,x_N} P(x_1,x_2,...,x_N)log_2 P(x_1,x_2,...,x_N) \\\\\n",
    "       &= -\\sum_{x_1,x_2,...,x_N}P(x_1)P(x_2)...P(x_N)log_2 (P(x_1)P(x_2)...P(x_N)) \\\\\n",
    "       & = -\\sum_{x_1,x_2,...,x_N}P(x_1)P(x_2)...P(x_N)\\left[\\log_2(p(x_1)) + \\log_2(p(x_2)) +...+ \\log_2(p(x_N))\\right] \\\\ \n",
    "       &= -\\sum_{x_2}P(x_2)...\\sum_{x_N}P(x_N)\\sum_{x_1}P(x_1)log_2 P(x_1) -\\sum_{x_1}P(x_1)...\\sum_{x_N}P(x_N)\\sum_{x_2}P(x_2)log_2 P(x_2) - ... -\\sum_{x_1}P(x_1)\\sum_{x_2}P(x_2)...\\sum_{x_N}P(x_N)log_2 P(x_N) \\\\\n",
    "       &= H(x_1) + H(x_2) +...+H(x_N)\n",
    "\\end{align}\n",
    "\n",
    "As a result, the joint entropy for N dice rolls is $N*(-log\\frac{1}{6}) = 2.58N (bits)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy per letter is  4.184600828019136 bits. This makes sense because the maximum entropy for a letter is -log2(1/26) = 4.7 bits when all the alphabet are with equal probability\n"
     ]
    }
   ],
   "source": [
    "# 1f\n",
    "\n",
    "mono = pd.read_table('english_monograms.txt', header=None, sep=' ', index_col=0)\n",
    "mono['Prob'] = mono[1]/sum(mono[1])\n",
    "mono['-PlogP'] = -mono['Prob']*np.log2(mono['Prob'])\n",
    "mono_entropy = sum(mono['-PlogP'])\n",
    " # you can also use the previously defined function entropy(mono) to get the same result as I demonstrate for 1i\n",
    "\n",
    "print(\"The entropy per letter is \",mono_entropy, \n",
    "      \"bits. This makes sense because the maximum entropy for a letter is -log2(1/26) = 4.7 bits when all the alphabet are with equal probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since the assumtion is that every letter is independent, H(L1,L2) = H(L1) + H(L2) = 4.184*2 = 8.368 (bits).Note that H(L) means entropy for a single letter\n"
     ]
    }
   ],
   "source": [
    "# 1g\n",
    "\n",
    "print(\"Since the assumtion is that every letter is independent, H(L1,L2) = H(L1) + H(L2) = 4.184*2 = 8.368 (bits).\" \n",
    "      \"Note that H(L) means entropy for a single letter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           1      Prob    -PlogP\n",
      "0                               \n",
      "E  529117365  0.120965  0.368622\n",
      "T  390965105  0.089381  0.311394\n",
      "A  374061888  0.085517  0.303384\n",
      "O  326627740  0.074673  0.279520\n",
      "I  320410057  0.073251  0.276231\n",
      "N  313720540  0.071722  0.272647\n",
      "S  294300210  0.067282  0.261972\n",
      "R  277000841  0.063327  0.252107\n",
      "H  216768975  0.049557  0.214818\n",
      "L  183996130  0.042065  0.192288\n",
      "D  169330528  0.038712  0.181600\n",
      "C  138416451  0.031644  0.157649\n",
      "U  117295780  0.026816  0.139999\n",
      "M  110504544  0.025263  0.134067\n",
      "F   95422055  0.021815  0.120387\n",
      "G   91258980  0.020863  0.116478\n",
      "P   90376747  0.020662  0.115641\n",
      "W   79843664  0.018254  0.105427\n",
      "Y   75294515  0.017214  0.100877\n",
      "B   70195826  0.016048  0.095669\n",
      "V   46337161  0.010593  0.069500\n",
      "K   35373464  0.008087  0.056206\n",
      "J    9613410  0.002198  0.019406\n",
      "X    8369915  0.001914  0.017278\n",
      "Z    4975847  0.001138  0.011125\n",
      "Q    4550166  0.001040  0.010308\n",
      "Base on the table above, we can calculate the probability of getting randomly generated combination: \n",
      "QU and UQ is the P(U)*P(Q) = P(Q)*P(U) = 0.001040*0.026816 =  2.8e-05\n",
      "HT and TH is the P(H)*P(T) = P(T)*P(H) = 0.089381*0.049557 =  0.0044\n",
      "The actual probabilties are P(TH) = 0.0271, P(QU) = 0.000964, which are enriched and P(HT) = 0.00193, P(UQ) = 5.41e-06,these are depleted.\n"
     ]
    }
   ],
   "source": [
    "# 1h\n",
    "\n",
    "print(mono)\n",
    "print(\"Base on the table above, we can calculate the probability of getting randomly generated combination: \")\n",
    "print(\"QU and UQ is the P(U)*P(Q) = P(Q)*P(U) = 0.001040*0.026816 = {0: .2g}\".format(mono.loc['Q','Prob']*mono.loc['U','Prob']))\n",
    "print(\"HT and TH is the P(H)*P(T) = P(T)*P(H) = 0.089381*0.049557 = {0: .2g}\".format(mono.loc['T','Prob']*mono.loc['H','Prob']))\n",
    "\n",
    "bigram = pd.read_table('english_bigrams.txt', header=None, sep=' ', index_col=0)\n",
    "bigram['Prob'] = bigram[1]/sum(bigram[1])\n",
    "print(\"The actual probabilties are P(TH) ={0: .3g}, P(QU) ={1: .3g}, which are enriched and P(HT) ={2: .3g}, P(UQ) ={3: .3g},these are depleted.\"\n",
    "      .format(bigram.loc['TH','Prob'],bigram.loc['QU','Prob'], bigram.loc['HT','Prob'],bigram.loc['UQ','Prob'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual entropy for bigram is 7.84 (bits) which is smaller than the independent assumtion calculated in 1g, 8.4 (bits).It's because that in real world, the letters in the bigram are not independent.\n"
     ]
    }
   ],
   "source": [
    "# 1i\n",
    "\n",
    "bi_entropy = entropy(bigram)\n",
    "print(\"The actual entropy for bigram is{0: .3g} (bits) which is smaller than the independent assumtion calculated in 1g,{1: .2g} (bits).\"\n",
    "      \"It's because that in real world, the letters in the bigram are not independent.\".format(bi_entropy, 2*mono_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information I(L1;L2), amount of information you can get for L2(L1) by knowing L1(L2), is defined as H(L1) + H(L2) - H(L1,L2) = 0.53 (bits)\n"
     ]
    }
   ],
   "source": [
    "# 1j\n",
    "\n",
    "print(\"Mutual information I(L1;L2), amount of information you can get for L2(L1) by knowing L1(L2),\"\n",
    "      \" is defined as H(L1) + H(L2) - H(L1,L2) ={0: .2g} (bits)\".format(2*mono_entropy-bi_entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.The prediction value for probability of word NEURON is 7.9e-08 and the actual value is 9.4e-07\n",
      "2.The prediction value for probability of word UCSD is 2.2e-06 and the actual value is 4.6e-07\n",
      "3.The log likelihood of word NEURON is 3.6 and UCSD is -2.3\n"
     ]
    }
   ],
   "source": [
    "# 1k\n",
    "# we'll use table mono again to compute the prediction entropy based on independent letters model\n",
    "\n",
    "word = pd.read_table('english_words.txt', header=None, sep=' ', index_col=0)\n",
    "word['Prob'] = word[1]/sum(word[1])\n",
    "neu_pre = mono.loc['N','Prob']*mono.loc['E','Prob']*mono.loc['U','Prob']*mono.loc['R','Prob']*mono.loc['O','Prob']*mono.loc['N','Prob']\n",
    "neu_act = word.loc['NEURON','Prob']\n",
    "ucsd_pre = mono.loc['U','Prob']*mono.loc['C','Prob']*mono.loc['S','Prob']*mono.loc['D','Prob']\n",
    "ucsd_act = word.loc['UCSD','Prob']\n",
    "print(\"1.The prediction value for probability of word NEURON is{0: .2g} and the actual value is{1: .2g}\".format(neu_pre,neu_act))\n",
    "print(\"2.The prediction value for probability of word UCSD is{0: .2g} and the actual value is{1: .2g}\".format(ucsd_pre,ucsd_act))\n",
    "print(\"3.The log likelihood of word NEURON is{0: .2g} and UCSD is {1: .2g}\".format(np.log2(neu_act/neu_pre),np.log2(ucsd_act/ucsd_pre)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a\n",
    "$p(n=1) = \\frac{1}{2}p(n=1|x=1) + \\frac{1}{2}p(n=1|x=2) = \\frac{1}{2}\\times0.04 + \\frac{1}{2} \\times 0.01 = 0.025$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x=1       x=2        \n",
      "n=0  P(x1,n0)  P(x2,n0)  P(n=0)\n",
      "n=1  P(x1,n1)  P(x2,n1)  P(n=1)\n",
      "       P(x=1)    P(x=2)        \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x=1</th>\n",
       "      <th>x=2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n=0</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x=1    x=2\n",
       "n=0  0.48  0.495\n",
       "n=1  0.02  0.005"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2b\n",
    "dt = 0.01\n",
    "r1 = 4\n",
    "r2 = 1\n",
    "P_x1 = 0.5 # P(x=1)\n",
    "P_x2 = 0.5 # P(x=2)\n",
    "P_n1x1 = r1*dt # P(n=1|x=1)\n",
    "P_n1x2 = r2*dt # P(n=1|x=2)\n",
    "P_n0x1 = 1-P_n1x1 # P(n=0|x=1) = 1 - P(n=1|x=1)\n",
    "P_n0x2 = 1-P_n1x2 # P(n=0|x=2) = 1 - P(n=1|x=2)\n",
    "\n",
    "print(pd.DataFrame([[\"P(x1,n0)\", \"P(x2,n0)\", \"P(n=0)\"], [\"P(x1,n1)\", \"P(x2,n1)\",\"P(n=1)\"],[\"P(x=1)\", \"P(x=2)\",\"\"]], \n",
    "                   columns=['x=1', 'x=2',\"\"], index=['n=0', 'n=1',\"\"]))\n",
    "df = pd.DataFrame([[P_x1*P_n0x1, P_x2*P_n0x2], [P_x1*P_n1x1, P_x2*P_n1x2]], columns=['x=1', 'x=2'], index=['n=0', 'n=1'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(x) = -P(x1)log2(P(x1))-P(x2)log2(P(x2)) = 1 (bits)\n",
      "H(n) = -P(n0)log2(P(n0))-P(n1)log2(P(n1)) = 0.169 (bits)\n",
      "H(x,n) = -P(x1,n0)log2(P(x1,n0))-P(x2,n0)log2(P(x2,n0))-P(x1,n1)log2(P(x1,n1))-P(x2,n1)log2(P(x2,n1)) = 1.16 (bits)\n"
     ]
    }
   ],
   "source": [
    "# 2c\n",
    "# read the table above would help\n",
    "\n",
    "H_x = entropy(df.sum(axis=0)) #= -P_x1*np.log2(P_x1)-P_x2*np.log2(P_x2)\n",
    "\n",
    "H_n = entropy(df.sum(axis=1))#-P_n0*np.log2(P_n0)-P_n1*np.log2(P_n1)\n",
    "\n",
    "H_nx = entropy(df)\n",
    "\n",
    "print(\"H(x) = -P(x1)log2(P(x1))-P(x2)log2(P(x2)) ={0: .3g} (bits)\".format(H_x))\n",
    "print(\"H(n) = -P(n0)log2(P(n0))-P(n1)log2(P(n1)) ={0: .3g} (bits)\".format(H_n))\n",
    "print(\"H(x,n) = -P(x1,n0)log2(P(x1,n0))-P(x2,n0)log2(P(x2,n0))-P(x1,n1)log2(P(x1,n1))-P(x2,n1)log2(P(x2,n1)) ={0: .3g} (bits)\".format(H_nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x=1    0.5\n",
       "x=2    0.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information I(x;n) = H(x) + H(n) - H(x,n) = 0.00712 (bits)\n"
     ]
    }
   ],
   "source": [
    "# 2d\n",
    "\n",
    "print(\"Mutual information I(x;n) = H(x) + H(n) - H(x,n) ={0: .3g} (bits)\".format(H_x+H_n-H_nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               x=1        x=2          \n",
      "n=(0,0)  P(x1,n00)  P(x2,n00)  P(n=0,0)\n",
      "n=(1,0)  P(x1,n10)  P(x2,n10)  P(n=1,0)\n",
      "n=(0,1)  P(x1,n01)  P(x2,n01)  P(n=0,1)\n",
      "n=(1,1)  P(x1,n11)  P(x2,n11)  P(n=1,1)\n",
      "            P(x=1)     P(x=2)          \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x=1</th>\n",
       "      <th>x=2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n=(0,0)</th>\n",
       "      <td>0.4802</td>\n",
       "      <td>0.495013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=(1,0)</th>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=(0,1)</th>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n=(1,1)</th>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            x=1       x=2\n",
       "n=(0,0)  0.4802  0.495013\n",
       "n=(1,0)  0.0098  0.002488\n",
       "n=(0,1)  0.0098  0.002488\n",
       "n=(1,1)  0.0002  0.000013"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2e\n",
    "# note that dt = 5ms now, different from previous questions\n",
    "dt = 0.005\n",
    "r1 = 4\n",
    "r2 = 1\n",
    "P_x1 = 0.5 # P(x=1)\n",
    "P_x2 = 0.5 # P(x=2)\n",
    "P_n1x1 = r1*dt # P(n=1|x=1)\n",
    "P_n1x2 = r2*dt # P(n=1|x=2)\n",
    "P_n0x1 = 1-P_n1x1 # P(n=0|x=1) = 1 - P(n=1|x=1)\n",
    "P_n0x2 = 1-P_n1x2 # P(n=0|x=2) = 1 - P(n=1|x=2)\n",
    "\n",
    "print(pd.DataFrame([[\"P(x1,n00)\", \"P(x2,n00)\", \"P(n=0,0)\"], [\"P(x1,n10)\", \"P(x2,n10)\",\"P(n=1,0)\"],[\"P(x1,n01)\", \"P(x2,n01)\", \"P(n=0,1)\"], [\"P(x1,n11)\", \"P(x2,n11)\",\"P(n=1,1)\"],[\"P(x=1)\", \"P(x=2)\",\"\"]], \n",
    "                   columns=['x=1', 'x=2',\"\"], index=['n=(0,0)', 'n=(1,0)', 'n=(0,1)', 'n=(1,1)',\"\"]))\n",
    "df2 = pd.DataFrame([[P_x1*P_n0x1*P_n0x1, P_x2*P_n0x2*P_n0x2], [P_x1*P_n0x1*P_n1x1, P_x2*P_n0x2*P_n1x2], [P_x1*P_n1x1*P_n0x1, P_x2*P_n1x2*P_n0x2], [P_x1*P_n1x1*P_n1x1, P_x2*P_n1x2*P_n1x2]], \n",
    "                   columns=['x=1', 'x=2'], index=['n=(0,0)', 'n=(1,0)', 'n=(0,1)', 'n=(1,1)'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(x,n12) = -P(x1,n0)log2(P(x1,n0))-P(x2,n0)log2(P(x2,n0))-P(x1,n1)log2(P(x1,n1))-P(x2,n1)log2(P(x2,n1)) = 1.1869 (bits)\n"
     ]
    }
   ],
   "source": [
    "# continue 2e\n",
    "H_xn12 = entropy(df2)\n",
    "print(\"H(x,n12) = -P(x1,n0)log2(P(x1,n0))-P(x2,n0)log2(P(x2,n0))-P(x1,n1)log2(P(x1,n1))-P(x2,n1)log2(P(x2,n1)) ={0: .5g} (bits)\".format(H_xn12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(n12) = 0.19388 (bits)\n"
     ]
    }
   ],
   "source": [
    "# 2f\n",
    "\n",
    "H_n12 = entropy(df2.sum(axis=1))\n",
    "print(\"H(n12) ={0: .5} (bits)\".format(H_n12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual information I(x;n12) = H(x) + H(n12) - H(x,n12) = 0.00702 (bits)\n"
     ]
    }
   ],
   "source": [
    "# 2g\n",
    "\n",
    "print(\"Mutual information I(x;n12) = H(x) + H(n12) - H(x,n12) ={0: .3g} (bits)\".format(H_x+H_n12-H_xn12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2h\n",
    "Mutual information I(x;n) means that by knowing one variable(x or n), how much information can you get for another variable(n or x). According to the results, $I(x, n) = 0.00712$ is close to $I(x, n_{12}) = 0.00702$, it means that n and $n_{12}$ give you the same amount of information about stimulus x which is counting spikes in a single time bin (10 ms) gives the same information as counting spikes in two time bins (5 ms each).This implies the neurons encode the stimulus by rate coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
